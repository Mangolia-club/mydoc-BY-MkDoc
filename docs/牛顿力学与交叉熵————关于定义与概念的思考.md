# 牛顿力学与交叉熵————关于定义与概念的思考

---

2021.11.10

---

## 概述

---

本文是今天学习了交叉熵的概念之后的一篇记录性的文章，原创性略低，来源主要是《数学之美》第6章——信息的度量与作用，里面讲解了信息论的基本知识：信息量、熵，又通过互信息引出了交叉熵。以及B站up主[王木头学科学](https://space.bilibili.com/504715181)的[“交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“KL散度”、“交叉熵”](https://www.bilibili.com/video/BV15V411W7VB)

---

## 前言

---

之前自学深度学习相关基础知识的时候学习到了交叉熵作为损失函数的概念，虽然对信息论、香农的一些贡献以及信息的熵略有耳闻，但是实在看不出为什么如下形式的交叉熵公式可以用来作为损失函数。

\[
\min -\left(\sum_{i=1}^{n}\left(x_{i} \cdot \log y_{i}+\left(1-x_{i}\right) \cdot \log \left(1-y_{i}\right)\right)\right)\tag{1}
\]

$$
交叉熵
$$

但是今天看《数学之美》的时候讲到了这个知识点，而且巧合的是在几天前正好看到了王木头讲的关于梯度下降的视频，他那种由问题步步深入引出关键知识点的教学方法让我对很多概念有了不同于书本上刻板的公式的更深的理解。于是又找出他讲的交叉熵的讲解视频，阅毕深有感触，以此文记录下当下的心情（所以并不是什么严谨的学术笔记）。

---

## 演绎

---

文章伊始，我想先将我今天的学习路径复述一次，以便之后的回顾。

**信息**，information.一个重要的作用就是减少我们对一件事情的不确定性。打开天气APP就可以看到接下来的天气怎么样，从而减少了对接下来的天气的不确定。但是信息的度量一直都没有相关的定义，类似古代人们知道一个人力量的大小但是不知道力具体的度量是怎样，只会粗浅的进行比较。

《数学之美》中这样讲到:

!!! cite
    「直到1948年，香农（Claude Shannon）在他著名的论文“通信的数学原理”（A Mathmatic Theory of Communication）中提出了“信息熵”的概念，才解决了信息的度量问题，并且量化出信息的作用。」

记得在讲softmax回归[（电子课件地址）](https://zh-v2.d2l.ai/chapter_linear-networks/softmax-regression.html?highlight=%E4%BA%A4%E5%8F%89%E7%86%B5)的时候，D2L cite:1 中对熵有这样的定义：

\[
H[P]=\sum_{j}-P(j) \log P(j)
\]

而类似的作为softmax回归的损失函数交叉熵损失（cross-entropy loss）的定义是这样的：

\[
l( y , \hat{ y })=-\sum_{j=1}^{q} y_{j} \log \hat{y}_{j}
\]

左边自不必说，代表自变量为标签值$y$以及模型预测值$\hat{y}$的损失（loss）函数。

右边则是先来一个不知所云的负号，然后一记西格玛冲拳就已经看的人头昏脑胀了，接下来又是一个标签值$y$乘以一个模型预测值$\hat{y}$的对数$\log \hat{y}$。大脑已经处理不了其中的逻辑宕机了。脑子中只剩两个问题：

!!! question
    为什么这样过分蹂躏我的公式可以表示的出标签和预测值的差距？

    为什么它会比最小二乘法要来得好以至于在这里使用？

要明白这些问题的答案光是看教材，半个世纪也不一定明白的比第一眼更多，所以咱们来先讲点故事。

继续讲到香农开创了信息论。那首先，信息怎么度量呢？

上文提到，信息重要的作用就是减少我们对一件事情的不确定性。那么信息量自然可以用减少的不确定性的量来定义。这也就引入了概率论的知识。

一个事件$x$发生的概率是$P(x)$，其值在$(0,1)$间，$P(x)$越高事件发生的可能性越高。比如简单地，掷一次硬币正面朝上的概率就是0.5。


如果掷两次硬币结果都是朝上，这样的事情发生的概率可以表示为：$P(正面朝上)\times P(正面朝上)=0.5^2=0.25$

而没有掷出去的时候我们不知道结果会是怎么样，但是一旦知道结果，比如正面朝上了，那么事情就是确定的了，从0.5变成了1。

其实在生活中有一种可以将概率和信息的价值直接联系起来的情况：博彩。

比如说在一个简单的规则中：赌骰子大小。押10个金币，猜骰子点数是大是小，猜中了就可以获得10个金币。

在这种情况中猜对猜错的概率都是0.5，如果我有未卜先知的能力，告诉你第一次的结果是大，那么这条信息就值20个金币。

而这盘掷两个骰子同时输赢的倍率翻倍的话，预言就值：$10\times 2^2 =40$个金币。但是这个时候这个答案所代表的结果发生的概率就降到了$0.5^2=0.25$。

这个时候我的预言的价值就随着游戏轮数呈指数上升：预言10局就值10240个金币！

虽然我并不能未卜先知，但是我知道我们如果想对某条信息的价值进行度量，信息量必须是和信息所代表的事件发生的概率相关。

比如我们可以定义一个函数$f(x)$。那么，“这次掷硬币的结果是正面。”这条信息的信息量就可以表达为：$f(0.5)$，即$f$表示从信息到信息量的函数，而自变量$x$是信息中蕴含的概率。

如果我们期待信息量是一种可以直接的（也就是线性）叠加的度量。


那信息量可以表达为第一个事件的信息量加上第二个事件的信息量：$f(0.25)=f(0.5)+f(0.5)$。

而概率上两个事件接连发生的概率等于两个事件发生概率的乘积。这时候我们就想到：这个函数里应该有$\log_{n}(P(x))$。

而信息的价值会随着概率降低而上升，所以还要有个负号。

但是对数的定义域并不包括负数，所以我们只能将目光转向其他的负相关的函数。

最后发现选择$\frac{1}{x}$是最合适的。

因为：

\[
\log _{n} M^{a}=a \log _{n} M
\]

也就是说$\log_{n}(\frac{1}{x})=-\log_{n}(x)$。

这不正是上面那个交叉熵定义中负号的来源嘛，坚定了我认识交叉熵的信心。

对数中的底数按理是不会对我们的度量产生任何影响的。但是众所周知，现在我们的计算机几乎都是基于冯诺依曼架构的，采用二进制编码。二进制存储对于计算机而言具有天然的优势，所以一般在信息论中采用2作为对数的底数。并且往往忽略底数2直接记作$\log n$.

现在这个定义式就变成了$f(P(x))=-\log(P(x))$，现在这个式子就是香农对于信息量的定义了。这时长呼一口气，总算是对一个信息的量有了概念。

那接下来让我康康信息熵又是什么！

熵这个概念对应的一般是一个系统，正如信息往往不是单独的不对其他事件有影响的：比如你听了我的预言两次都押对了，但是你的对手们却遭殃了，所以我的预言对他们也是有价值的。

比如说如果不知道事情的结果，两个骰子选和你选择不一样的话，你输而他赢的概率有：0.75。对于他而言，他赢的信息量就是$-\log(0.75)$。很显然比对你的价值低。

而对系统的而言，确定性高得多，因为结果上看你输的概率很大，比单掷一个的时候“确定”的多。

那对于这两局游戏来说，整体的信息量，也就是系统的信息熵如何呢？

是两者简单加和吗？

如果是的话，那么单掷一个骰子的整体信息熵会是：$2\times-log\frac{1}{2}=2$

而掷两个骰子的整体信息熵会是：$-\log(0.75)-\log(0.25)\approx 2.415$。明明对于整体而言，结果是更加的确定的，这和我们对信息熵的期待不符。

小学的时候我们学过加权平均数的概念：将各数值乘以相应的权数，然后加总求和得到总体值，再除以总的单位数。

因为每一个数值对整个系统的影响大小并不像每一分钱对自己财富的影响一样一致。每一条信息对整个系统的影响也是有权重的。

即每个事件并不是同等概率发生的。就像是即使任何一个时刻之后都有可能发生意外，但是这并不是总会使我们在任何时刻都谨小慎微，只有在面对的风险比较大，也就是发生的概率较大的危险事件，才会打起百分之八百的专注。

对于一个系统而言，每条信息对于系统的贡献值必然和它发生的概率是相关的，也就是说信息熵的函数可以表示为：$H(P(x),-\log(P(x)))$，当然，在我们了解了具体在函数中两部分的关系，就可以将这个二元函数转换成一元函数。

加权平均数里，权重和对应的数值是相乘的关系，自然我们就会联想会不会在这里也是相乘的关系？

而权重代表的意思正是取决于各数值出现的次数（频数），和概率不能说一模一样，只能说是巧巧他妈给巧巧开门————巧到家了。

于是我们构建的信息熵